# 添加层

tensorflow我们要做的主要就是添加两个隐藏层。layer1和layer2

这一节总觉得没咋看懂，总结以后写吧
---


```python
import tensorflow as tf
import numpy as np

def add_layer(inputs, in_size, out_size, activation_function=None): #默认activation_function为None
    Weights = tf.Variable(tf.random_normal([in_size,out_size])) #矩阵，随机变量，形状是[in_size,out_size]
    biases = tf.Variable(tf.zeros([1,out_size]) + 0.1) #列表，biases尽量不要为0
    Wx_plus_b = tf.matmul(inputs, Weights) + biases
    if activation_function is None:
        outputs = Wx_plus_b
    else:
        outputs = activation_function(Wx_plus_b)
    return outputs

x_data = np.linspace(-1,1,300)[:,np.newaxis]  #加一个维度，300行，都在-1和1之间，linspace是一个等差数列
noise = np.random.normal(0,0.05,x_data.shape)   #方差0.05，跟x_data一样的格式
y_data = np.square(x_data) - 0.5 + noise

xs=tf.placeholder(tf.float32,[None, 1])
ys=tf.placeholder(tf.float32,[None, 1])

l1 = add_layer(xs,1,10,activation_function=tf.nn.relu)  #输入层有一个，隐藏层有10个
prediction = add_layer(l1,10,1,activation_function=None) #隐藏层有10个，输出层有1个

loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices = [1]))
#对每个例子进行求和再求平均值

train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
#学习率要小于1，对误差进行更正

init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

for i in range(1000):
    sess.run(train_step,feed_dict={xs:x_data,ys:y_data})
    #run1000次train_step
    if i %50 ==0:
        print(sess.run(loss,feed_dict={xs:x_data,ys:y_data}))
        #打印loss值

```
## plot可视化

Matplotlib对象简介

   FigureCanvas  画布

   Figure        图

   Axes          坐标轴(实际画图的地方)

像本例中一样
`fig = plt.figure()`  #添加一个图片框
`ax = fig.add_subplot(1,1,1)` #子图：就是在图片框里面生成多张子图。
`ax.scatter(x_data, y_data)` #以x_data和y_data作为散点
`plt.show()` #显示


